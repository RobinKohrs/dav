closest_idx <- which.min(abs(catalog_clean$parsed_date - date_start))
closest_idx
target_files <- catalog_clean[closest_idx, , drop = FALSE]
target_files
target_files %>% glimpse
target_files %>% View
start_date = as.Date("2025-10-30")
date_start <- lubridate::as_date(date_start)
# 2. Filter Logic
target_files <- NULL
# Logic: Find CLOSEST single date
message(paste0("Searching for single image closest to ", date_start, "..."))
closest_idx <- which.min(abs(catalog_clean$parsed_date - date_start))
target_files <- catalog_clean[closest_idx, , drop = FALSE]
View(target_files)
date_start
date_start = as.Date("2025-10-30")
date_start
date_start <- lubridate::as_date(date_start)
# 2. Filter Logic
target_files <- NULL
# Logic: Find CLOSEST single date
message(paste0("Searching for single image closest to ", date_start, "..."))
closest_idx <- which.min(abs(catalog_clean$parsed_date - date_start))
target_files <- catalog_clean[closest_idx, , drop = FALSE]
target_files
target_files$content_date_end
diff_days <- abs(target_files$parsed_date - date_start)
diff_days
message(paste0(
"Found image from ",
target_files$parsed_date,
" (",
diff_days,
" days difference)"
))
target_files <- catalog_clean %>%
dplyr::filter(parsed_date >= date_start & parsed_date <= date_end)
# 2. Filter Logic
target_files <- NULL
if (is.null(date_end)) {
# Logic: Find CLOSEST single date
message(paste0("Searching for single image closest to ", date_start, "..."))
closest_idx <- which.min(abs(catalog_clean$parsed_date - date_start))
target_files <- catalog_clean[closest_idx, , drop = FALSE]
diff_days <- abs(target_files$parsed_date - date_start)
message(paste0(
"Found image from ",
target_files$parsed_date,
" (",
diff_days,
" days difference)"
))
} else {
# Logic: Find ALL images in range
date_end <- lubridate::as_date(date_end)
message(paste0(
"Searching for images between ",
date_start,
" and ",
date_end,
"..."
))
target_files <- catalog_clean %>%
dplyr::filter(parsed_date >= date_start & parsed_date <= date_end)
message(paste0("Found ", nrow(target_files), " images in range."))
}
target_files
if (nrow(target_files) == 0) {
warning("No files found matching criteria.")
return(invisible(NULL))
}
processed_files <- c()
i = 1
row <- target_files[i, ]
row
s3_full_path <- row$s3_path
s3_full_path
# Parse S3 URL
parts <- strsplit(s3_full_path, "/")[[1]]
parts
# s3://BUCKET/KEY -> parts: "s3:", "", "BUCKET", "KEY", ...
if (length(parts) < 4) {
warning(paste("Invalid S3 path format:", s3_full_path))
next
}
bucket_name <- parts[3]
bucket_name
object_key <- paste(parts[4:length(parts)], collapse = "/")
object_key
# Use a unique temp filename to avoid collisions if parallelizing later
temp_file_path <- file.path(
output_dir,
paste0("temp_", Sys.getpid(), "_", original_filename)
)
original_filename <- basename(s3_full_path)
# Use a unique temp filename to avoid collisions if parallelizing later
temp_file_path <- file.path(
output_dir,
paste0("temp_", Sys.getpid(), "_", original_filename)
)
temp_file_path
# Create final filename
file_stub <- tools::file_path_sans_ext(original_filename)
file_stub
# Append date to ensure uniqueness if filename doesn't have it
final_file_path <- file.path(output_dir, paste0(file_stub, "_clipped.tif"))
final_file_path
message(paste0(
"[",
i,
"/",
nrow(target_files),
"] Processing: ",
row$parsed_date
))
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = "" # CDSE doesn't use standard regions usually
)
# A. Download to Temp
# Pass base_url for CDSE
# Explicitly set parameters to avoid common signature/path errors with CDSE
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = "default", # CDSE often uses 'default' or 'RegionOne' or ignores it, but default in aws.s3 is us-east-1
use_https = TRUE,
check_region = FALSE # Important: prevents aws.s3 from trying to look up the bucket region which fails on CDSE
)
?aws.s3::save_object()
# Handle hostname resolution issue by ensuring proper style
# path_style = TRUE is often needed for non-AWS S3 to avoid prepending bucket to hostname
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = "default",
use_https = TRUE,
check_region = FALSE,
path_style = TRUE
)
bucket_name
s3_endpoint
# Handle hostname resolution issue by ensuring proper style
# path_style = TRUE is often needed for non-AWS S3 to avoid prepending bucket to hostname
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = "default",
use_https = TRUE,
check_region = FALSE,
path_style = TRUE
)
# Handle hostname resolution issue by ensuring proper style
# path_style = TRUE is often needed for non-AWS S3 to avoid prepending bucket to hostname
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = ""
use_https = TRUE,
# Handle hostname resolution issue by ensuring proper style
# path_style = TRUE is often needed for non-AWS S3 to avoid prepending bucket to hostname
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = "",
use_https = TRUE,
check_region = FALSE,
path_style = TRUE
)
object_key
bucket_name
temp_file_path
s3_endpoint
# Check if bucket name needs to be lowercase "eodata" for CDSE?
# The S3 protocol is case sensitive for keys but buckets are usually lowercase in DNS.
# Let's force bucket to lowercase if it is "EODATA" just in case,
# though standard S3 allows mixed case if not via DNS.
if (bucket_name == "EODATA") {
bucket_name <- "eodata"
}
original_filename <- basename(s3_full_path)
# Use a unique temp filename to avoid collisions if parallelizing later
temp_file_path <- file.path(
output_dir,
paste0("temp_", Sys.getpid(), "_", original_filename)
)
# Create final filename
file_stub <- tools::file_path_sans_ext(original_filename)
# Append date to ensure uniqueness if filename doesn't have it
final_file_path <- file.path(output_dir, paste0(file_stub, "_clipped.tif"))
message(paste0(
"[",
i,
"/",
nrow(target_files),
"] Processing: ",
row$parsed_date
))
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = "", # Empty string is safer than "default" for some backends
use_https = TRUE,
check_region = FALSE,
path_style = TRUE
)
bucket_name
bucket_name ="EODATA"
aws.s3::save_object(
object = object_key,
bucket = bucket_name,
file = temp_file_path,
base_url = s3_endpoint,
region = "",
use_https = TRUE,
check_region = FALSE,
path_style = TRUE
)
temp_file_path
s3_endpoint
object_key
format = "cog"
if (nrow(target_files) == 0) {
return(invisible(NULL))
}
processed_files <- c()
i
row <- target_files[i, ]
s3_raw_path <- row$s3_path
s3_raw_path
devtools::load_all()
cdse_list_ndvi_products()
cdse_list_ndvi_products() %>% gliipse
cdse_list_ndvi_products() %>% glimpse
cdse_list_ndvi_products() -> a
devtools::load_all()
cdse_list_ndvi_products() -> a
a$csv_url
collection = "ndvi_global_300m_10daily_v2"
format = "cog"
access_key
if (!format %in% c("cog", "nc")) {
stop("format must be 'cog' or 'nc'")
}
# Resolve Collection to CSV URL
products <- .cdse_ndvi_products()
if (!collection %in% products$collection) {
stop(
"Invalid collection. Run `cdse_list_ndvi_products()` to see available options."
)
}
prod_info <- products[products$collection == collection, ]
csv_resources <- prod_info$csv_url
csv_resources
# Check if requested format is likely supported (soft check)
if (format == "cog" && !grepl("cog", prod_info$format_avail)) {
warning(paste(
"Collection",
collection,
"mostly contains NetCDF (.nc). 'cog' format might not be available."
))
}
# Set credentials for session
if (!is.null(access_key)) {
Sys.setenv("AWS_ACCESS_KEY_ID" = access_key)
}
if (!is.null(secret_key)) {
Sys.setenv("AWS_SECRET_ACCESS_KEY" = secret_key)
}
# CDSE Endpoint Constant
S3_ENDPOINT <- "eodata.dataspace.copernicus.eu"
message(paste("--- Accessing Catalog:", collection, "---"))
full_catalog <- tryCatch(
{
dplyr::bind_rows(lapply(csv_resources, function(x) {
# read_delim handles URLs automatically
readr::read_delim(x, delim = ";", show_col_types = FALSE)
}))
},
error = function(e) stop("Failed reading catalog CSV: ", e$message)
)
if (nrow(full_catalog) == 0) {
stop("No data found in catalog.")
}
# Normalize columns
names(full_catalog) <- tolower(names(full_catalog))
# Find date column
if (!"nominal_date" %in% names(full_catalog)) {
date_col <- names(full_catalog)[grepl("date|time", names(full_catalog))][1]
if (is.na(date_col)) {
stop("No date column found in CSV")
}
full_catalog$nominal_date <- full_catalog[[date_col]]
}
if (!"s3_path" %in% names(full_catalog)) {
stop("No 's3_path' column in CSV")
}
# Filter Data
catalog_clean <- full_catalog %>%
dplyr::mutate(
parsed_date = lubridate::as_date(nominal_date),
s3_path = as.character(s3_path)
) %>%
dplyr::filter(!is.na(parsed_date))
catalog_clean
date_start <- lubridate::as_date(date_start)
if (is.null(date_end)) {
message(paste("Searching closest image to", date_start))
idx <- which.min(abs(catalog_clean$parsed_date - date_start))
target_files <- catalog_clean[idx, , drop = FALSE]
message(paste("Selected date:", target_files$parsed_date))
} else {
date_end <- lubridate::as_date(date_end)
message(paste("Searching range", date_start, "to", date_end))
target_files <- catalog_clean %>%
dplyr::filter(parsed_date >= date_start & parsed_date <= date_end)
message(paste("Found", nrow(target_files), "images."))
}
if (nrow(target_files) == 0) {
return(invisible(NULL))
}
target_files
# Ensure output directory exists
if (!dir.exists(output_dir)) {
dir.create(output_dir, recursive = TRUE)
}
processed_files <- c()
i
row <- target_files[i, ]
s3_raw_path <- row$s3_path
# A. Parse Bucket/Prefix
parts <- strsplit(s3_raw_path, "/")[[1]]
# FORCE bucket to lowercase 'eodata' (Crucial for CDSE)
bucket_name <- "eodata"
# Extract the directory prefix from CSV
# Usually parts: "s3:", "", "EODATA", "CLMS", ...
folder_prefix <- paste(parts[4:length(parts)], collapse = "/")
folder_prefix
# B. Smart Path Adjustment (Swap _cog/_nc based on request)
# This logic mainly applies to the 300m v2 collection which has dual formats side-by-side
if (format == "nc" && grepl("_cog$", folder_prefix)) {
folder_prefix <- sub("_cog$", "_nc", folder_prefix)
} else if (format == "cog" && grepl("_nc$", folder_prefix)) {
folder_prefix <- sub("_nc$", "_cog", folder_prefix)
}
folder_prefix
message(paste0(
"[",
i,
"/",
nrow(target_files),
"] Checking folder: ",
folder_prefix
))
items <- aws.s3::get_bucket(
bucket = bucket_name,
prefix = folder_prefix,
base_url = S3_ENDPOINT,
region = "",
use_https = TRUE,
check_region = FALSE,
path_style = TRUE
)
items
keys <- sapply(items, function(x) x$Key)
keys
# D. Filter for the specific file type
target_key <- NULL
if (format == "cog") {
# Look for .tiff AND "NDVI" (to skip quality flags)
# Use regex: contains "NDVI" (case insensitive) and ends in .tif/.tiff
matches <- keys[grepl(
"NDVI.*\\.tiff?$|NDVI.*\\.tif$",
keys,
ignore.case = TRUE
)]
if (length(matches) > 0) target_key <- matches[1]
} else if (format == "nc") {
# Look for .nc
matches <- keys[grepl("\\.nc$", keys, ignore.case = TRUE)]
if (length(matches) > 0) target_key <- matches[1]
}
target_key
if (is.null(target_key)) {
message("  -> Warning: Target format not found in folder. Skipping.")
next
}
message(paste("  -> Found:", basename(target_key)))
# E. Download
original_filename <- basename(target_key)
temp_file <- file.path(
output_dir,
paste0("temp_", Sys.getpid(), "_", original_filename)
)
temp_file
aws.s3::save_object(
object = target_key,
bucket = bucket_name,
file = temp_file,
base_url = S3_ENDPOINT,
region = "",
use_https = TRUE,
path_style = TRUE,
check_region = FALSE
)
profile = "copernicus"
file = "~/.aws/credentials"
path <- path.expand(file)
path
if (!file.exists(path)) {
warning(paste("Credentials file not found at:", path))
return(NULL)
}
# Read the file line by line
lines <- readLines(path, warn = FALSE)
lines
# Find the profile section
# Look for [profile_name]
profile_header <- paste0("[", profile, "]")
start_idx <- which(trimws(lines) == profile_header)
start_idx
profile_header
if (length(start_idx) == 0) {
warning(paste("Profile", profile, "not found in", path))
return(NULL)
}
# Read lines after the profile header until the next section or end of file
# Initialize keys
access_key <- NULL
secret_key <- NULL
# Iterate through lines starting after the profile header
# We stop if we hit another header [...] or end of lines
for (i in (start_idx + 1):length(lines)) {
line <- trimws(lines[i])
# Stop if empty line or next section
if (line == "" || grepl("^\\[.*\\]$", line)) {
if (i > start_idx + 1 && !is.null(access_key)) break # optimization
if (grepl("^\\[.*\\]$", line)) break
next
}
# Parse key=value
if (grepl("^aws_access_key_id", line, ignore.case = TRUE)) {
access_key <- trimws(sub("^[^=]+=", "", line))
} else if (grepl("^aws_secret_access_key", line, ignore.case = TRUE)) {
secret_key <- trimws(sub("^[^=]+=", "", line))
}
}
access_key
secret_key
devtools::load_all()
library(tidyverse)
library(here)
library(glue)
library(sf)
library(davR)
library(jsonlite)
library(tidyverse)
library(here)
library(glue)
library(sf)
library(jsonlite)
devtools::load_all()
# ++++++++++++++++++++++++++++++
# download ----
# ++++++++++++++++++++++++++++++
geo_de = giscoR::gisco_get_nuts(country="DE")
geo_de
# ++++++++++++++++++++++++++++++
# download ----
# ++++++++++++++++++++++++++++++
geo_de = giscoR::gisco_get_nuts(country="DE") %>% filter(NUTS_NAME="HAMBURG")
# ++++++++++++++++++++++++++++++
# download ----
# ++++++++++++++++++++++++++++++
geo_de = giscoR::gisco_get_nuts(country="DE") %>% filter(NUTS_NAME=="HAMBURG")
plot(geo_de[0])
# ++++++++++++++++++++++++++++++
# download ----
# ++++++++++++++++++++++++++++++
geo_hh = giscoR::gisco_get_nuts(country="DE") %>% filter(NUTS_NAME=="HAMBURG")
# ++++++++++++++++++++++++++++++
# download most recent ndvi for hamburg ----
# ++++++++++++++++++++++++++++++
d_keys = cdse_set_credentials()
d_keys
# ++++++++++++++++++++++++++++++
# download most recent ndvi for hamburg ----
# ++++++++++++++++++++++++++++++
d_keys = cdse_set_credentials()
cdse_download_ndvi(date_start = Sys.Date(),clipsrc = geo_hh,output_dir = "~/LIXO/",access_key = d_keys$access_key, secret_key = d_keys$secret_key)
write_sf(geo_hh, "~/LIXO/hh.gpkg")
geo_hh
write_sf(geo_hh %>% st_as_sf(), "~/LIXO/hh.gpkg")
write_sf(geo_hh %>% st_as_sf() %>% select(NUTS_ID), "~/LIXO/hh.gpkg")
write_sf(geo_hh %>% st_as_sf() %>% select(NUTS_ID), "~/LIXO/hh.fgb")
devtools::load_all()
